{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "eAGVzCDP26bK"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import numpy as np\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eval_interval = 500\n",
        "learning_rate = 2e-4\n",
        "batch_size = 64\n",
        "block_size = 256\n",
        "max_iters = 6000\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 256\n",
        "n_head = 8\n",
        "n_layer = 6\n",
        "dropout = 0.1\n",
        "\n",
        "with open('adventures_1MB.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()"
      ],
      "metadata": {
        "id": "EHaE6a9O3Gnk"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "le = LabelEncoder()\n",
        "le.fit(list(set(text)))\n",
        "vocab_size = len(le.classes_)\n",
        "\n",
        "encoded_data = le.transform(list(text))\n",
        "data = torch.tensor(encoded_data, dtype=torch.long)\n",
        "\n",
        "train_data_np, val_data_np = train_test_split(data.numpy(), test_size=0.1, shuffle=False)\n",
        "train_data = torch.tensor(train_data_np, dtype=torch.long)\n",
        "val_data = torch.tensor(val_data_np, dtype=torch.long)\n",
        "\n",
        "def get_batch(split):\n",
        "    split_data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(split_data) - block_size, (batch_size,))\n",
        "    x = torch.stack([split_data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([split_data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            _, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "class Head(nn.Module):\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)\n",
        "        q = self.query(x)\n",
        "        wei = q @ k.transpose(-2,-1) * C**-0.5\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
        "        wei = F.softmax(wei, dim=-1)\n",
        "        wei = self.dropout(wei)\n",
        "        v = self.value(x)\n",
        "        out = wei @ v\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n",
        "\n",
        "class FeedFoward(nn.Module):\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd)\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "        tok_emb = self.token_embedding_table(idx)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n",
        "        x = tok_emb + pos_emb\n",
        "        x = self.blocks(x)\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.lm_head(x)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        for _ in range(max_new_tokens):\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            logits, _ = self(idx_cond)\n",
        "            logits = logits[:, -1, :]\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "        return idx\n",
        "\n",
        "model = BigramLanguageModel().to(device)\n",
        "print(sum(p.numel() for p in model.parameters()) / 1e6, 'M parameters')\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "    xb, yb = get_batch('train')\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "context = torch.ones((1, 1), dtype=torch.long, device=device)\n",
        "output = model.generate(context, max_new_tokens=2000)[0].tolist()\n",
        "print(le.inverse_transform(output))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "90WIbvUg3R6d",
        "outputId": "2f4c4448-f164-476c-a6f1-a3121fe447d0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4.84617 M parameters\n",
            "step 0: train loss 4.6316, val loss 4.6286\n",
            "step 500: train loss 2.3781, val loss 2.4656\n",
            "step 1000: train loss 1.9228, val loss 2.0388\n",
            "step 1500: train loss 1.6776, val loss 1.8050\n",
            "step 2000: train loss 1.5160, val loss 1.6538\n",
            "step 2500: train loss 1.4089, val loss 1.5772\n",
            "step 3000: train loss 1.3342, val loss 1.5163\n",
            "step 3500: train loss 1.2729, val loss 1.4769\n",
            "step 4000: train loss 1.2305, val loss 1.4472\n",
            "step 4500: train loss 1.1938, val loss 1.4331\n",
            "step 5000: train loss 1.1660, val loss 1.4160\n",
            "step 5500: train loss 1.1351, val loss 1.4017\n",
            "step 5999: train loss 1.1127, val loss 1.3962\n",
            "['\\n' 'a' 'n' ... 'u' 't' ' ']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# prompt = \"The \"\n",
        "\n",
        "# input_ids = le.transform(list(prompt))\n",
        "# input_tensor = torch.tensor([input_ids], dtype=torch.long).to(device)\n",
        "\n",
        "\n",
        "# generated = model.generate(input_tensor, max_new_tokens=100)[0].tolist()\n",
        "\n",
        "# output_text = ''.join(le.inverse_transform(generated))\n",
        "\n",
        "# print(output_text)"
      ],
      "metadata": {
        "id": "pOuyVH1NeyxN"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OSrJ25INe5wU"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "prompt = \"The\"\n",
        "\n",
        "input_ids = le.transform(list(prompt))\n",
        "input_tensor = torch.tensor([input_ids], dtype=torch.long).to(device)\n",
        "\n",
        "\n",
        "generated = model.generate(input_tensor, max_new_tokens=1500)[0].tolist()\n",
        "\n",
        "output_text = ''.join(le.inverse_transform(generated))\n",
        "\n",
        "print(output_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5e213bd-8747-42e4-93a0-18b24f8a67ae",
        "id": "XRqSzPdue7yY"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2225826372.py:4: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)\n",
            "  input_tensor = torch.tensor([input_ids], dtype=torch.long).to(device)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thence,\n",
            "in the same abjourney according intested, my inconrimation with In a\n",
            "rein agest any Bacgan indulgence his abitation and heaven! Duly\n",
            "laugh without situation reat a grow mother astonishment.\n",
            "\n",
            "The captain too work, atmentipies the presumption of a partner, who\n",
            "acquainted me with the trust trick and offerrience, to waise sent to\n",
            "that were styleman. “Randol do you gaftin!” connoly prejudiced the goose\n",
            "upon me now; thip you gostly scarces, “Are not a mallices, ladieus\n",
            "young in the gentleman, before Strap’s summonically, know, d—you\n",
            "supppos with sendedom of adventure to paid her, I must in on this markablim\n",
            "eyes too man a few hotmer and barbarkfames.” I; asgemed the looks of releat.\n",
            "As I have in the foot of light, but she lipped his wildness of\n",
            "ay, and smile I became so minto eagh Thouffle by the Wiffe?” continued\n",
            "and Lord Mons. That explained with keachban each characted a same begon\n",
            "us betrange, by the mistaken is a disappointment of which we in escampes\n",
            "to each) on mentioned on the conversation with her and lacks: Did,\n",
            "therefore shillings be mean the gintle, and afraid which I returned, I hope\n",
            "swore the start of a sitty and never embocken and his incensument.\n",
            "\n",
            "My marty and custring drinked my (whom I was exployed and plexens with\n",
            "argument. I was replianted against my toran appect that not she duelible\n",
            "piece. Afforts, triunhed discovered me before then, rather first adjucted\n",
            "me, and had expressed forceedly firing me out adresses, in putrimble\n",
            "suat at aston the time, and aske\n"
          ]
        }
      ]
    }
  ]
}